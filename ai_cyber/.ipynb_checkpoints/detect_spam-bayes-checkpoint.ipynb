{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure the script is in the same directory as the Training and testing folders.\n",
      "Found the datasets.\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "Training the model...\n",
      "Found pickle file. Skipping training\n",
      "Testing on both training and testing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7950edfda17434587c7bf748769cd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75419), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuba\\Anaconda3\\envs\\latest\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Scores:\n",
      "F_score         : 0.44444\n",
      "False Negatives : 0.0\n",
      "False Positives : 2.0\n",
      "Precision       : 0.4\n",
      "Recall          : 0.5\n",
      "\n",
      "Testing Scores: \n",
      "F_score         : 0.45\n",
      "False Negatives : 0.0\n",
      "False Positives : 4.0\n",
      "Precision       : 0.40909\n",
      "Recall          : 0.5\n",
      "\n",
      "Combined Scores: \n",
      "F_score         : 0.39962\n",
      "False Negatives : 0.0\n",
      "False Positives : 2.522e+04\n",
      "Precision       : 0.3328\n",
      "Recall          : 0.5\n",
      "Results file created: C:\\Users\\chuba\\Documents\\Experiment\\NOVAHackathon\\ai_cyber\\NBresults.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Spam filter using Naive Bayes classifier\"\"\"\n",
    "\n",
    "\n",
    "import email.parser \n",
    "import os, sys, stat\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re, cgi\n",
    "import math, pickle\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "\n",
    "\n",
    "def extract_content(filename):\n",
    "\t''' Extract the subject and payload from the .eml file.'''\n",
    "\twith open(filename, 'rb') as fp:\n",
    "\t\tmsg = email.message_from_bytes(fp.read())\n",
    "\tsub = msg.get('subject')\n",
    "\t#If it is a multipart message, get_payload returns a list of parts.\n",
    "\tif msg.is_multipart():\n",
    "\t\tpayload = msg.get_payload()[0]\t\n",
    "\t\tpayload = payload.as_bytes() #We will consider the body as bytes so it is easier to decode into a unicode string.\n",
    "\telse:\n",
    "\t\tpayload =  msg.get_payload()\n",
    "\treturn \"{}\\n{}\" . format(sub, payload)\n",
    "\n",
    "def get_text_from_email(mail):\n",
    "\t\"\"\" Removes html tags and punctuations.\"\"\"\n",
    "\ttag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
    "\n",
    "\t# Remove well-formed tags, fixing mistakes by legitimate users\n",
    "\tmail = tag_re.sub('', mail)\n",
    "\n",
    "\t# Clean up anything else by escaping\n",
    "\tmail = html.escape(mail)\n",
    "\t\n",
    "\tmail = re.sub(r'([\\\\][n|t|x])', ' ', mail)                           #Removes \\n\\t\\b strings\n",
    "\tmail = re.sub(r'[=*/&;.,/\\\" ?:<>\\[\\]\\(\\)\\{\\}\\|%#`~\\\\]', ' ', mail)   #Removes punctuations\n",
    "\tmail = re.sub(r'[- _=+]{2,}|(?=\\s)[-_]|[-_](?=\\s)', ' ', mail)       #Removes unnecessary hiphens and underscores\n",
    "\tmail = re.sub(r'[\\d]', ' ', mail)                                    #Revoves all digits\n",
    "\tmail = re.sub(r'[\\'!=+]', '', mail)                                  #Replaces these punctuations with null string\n",
    "\treturn mail.lower()\n",
    "\n",
    "\n",
    "def preprocess(mail):\n",
    "\t\"\"\"Preprocess data\"\"\"\n",
    "\t# Currently just one preprocessing step.\n",
    "\tmail = get_text_from_email(mail)\n",
    "\treturn mail\n",
    "\n",
    "\n",
    "def add_words_to_dict(word_set, word_dict, ham):\n",
    "\t\"\"\"Checks if the word is presnt or not and increments its respective value\"\"\"\n",
    "\tfor word in word_set:\n",
    "\t\tif word not in word_dict:\n",
    "\t\t\tword_dict[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "\t\tif ham:\n",
    "\t\t\tword_dict[word]['ham_count'] = word_dict[word]['ham_count'] + 1\n",
    "\t\telse:\n",
    "\t\t\tword_dict[word]['spam_count'] = word_dict[word]['spam_count'] + 1 \n",
    "\n",
    "def calculate_spaminess(word, word_dict, total_ham, total_spam):\n",
    "\t\"\"\" Calculate the probability of a message being spam provided that the word is present.\"\"\"\n",
    "\n",
    "\tpr_s, pr_h = 0.5, 0.5  #Assumming equal probability for both ham and spam\n",
    "\tthreshold = 2   #Strength factor to handle rare words\n",
    "\ttotal_occurance = word_dict[word]['spam_count'] + word_dict[word]['ham_count']  #Total number of times the word has occured in both ham and spam\n",
    "\tfreq_s = word_dict[word]['spam_count'] / total_spam \n",
    "\tfreq_h = word_dict[word]['ham_count'] / total_ham\n",
    "\tspamminess = (freq_s * pr_s) / (freq_s * pr_s + freq_h * pr_h)  #The probability that a given mail is spam, provided that this word is present.\n",
    "\tcorrected_spaminess = (0.3 * threshold + total_occurance * spamminess) / (threshold + total_occurance)  #Considering the strength factor.\n",
    "\tword_dict[word]['spaminess'] = corrected_spaminess   \n",
    "\n",
    "def generate_dictionary(files, labels):\n",
    "\t\"\"\"Generates a dictionary of all the words in both ham and spam mails\"\"\"\n",
    "\t#Initializing variables\n",
    "\titerator = 0\n",
    "\tword_dict = {}\n",
    "\ttotal_spam = 0\n",
    "\ttotal_ham = 0\n",
    "\n",
    "\tfor file in tqdm(files):\n",
    "\t\t#Read and extract mail contents\n",
    "\t\ttry:\n",
    "\t\t\tmail = extract_content(file)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Corrupted File {}\" . format(file))\n",
    "\t\t# Prepare data\n",
    "\t\tmail = preprocess(mail)\n",
    "\t\tword_list = [s for s in mail.split()]\n",
    "\t\tword_set = set(word_list)\n",
    "\n",
    "\t\t# Incrementing HAM/SPAM count\n",
    "\t\tham = (True if int(labels[iterator]) == 1 else False)\n",
    "\t\tif ham:\n",
    "\t\t\ttotal_ham += 1\n",
    "\t\telse:\n",
    "\t\t\ttotal_spam += 1\n",
    "\n",
    "\t\tadd_words_to_dict(word_set, word_dict, ham)\n",
    "\t\titerator += 1\n",
    "\tfor word in word_dict:\n",
    "\t\tcalculate_spaminess(word, word_dict, total_ham, total_spam)\n",
    "\twith open('word_dict.pickle', 'wb') as f:\n",
    "\t\tpickle.dump(word_dict, f)\n",
    "\treturn word_dict\n",
    "\n",
    "def get_scores(expected, predicted):\n",
    "\t\"\"\" Compares predicted and expected values and returns various metrics.\"\"\"\n",
    "\tscores = {}\n",
    "\t# _ implies we do not care about that metric.\n",
    "\t_, scores['False Positives'], scores['False Negatives'], _ = confusion_matrix(expected, predicted).ravel()\n",
    "\tscores['Precision'], scores['Recall'], scores['F_score'], _ = precision_recall_fscore_support(expected, predicted, average='macro')\n",
    "\treturn scores\n",
    "\n",
    "def training(files, labels):\n",
    "\t\"\"\"Trains the model and returns a word dictionary\"\"\"\n",
    "\ttry:\n",
    "\t\twith open('word_dict.pickle', 'rb') as f:\n",
    "\t\t\tprint(\"Found pickle file. Skipping training\")\n",
    "\t\t\tword_dict = pickle.load(f)\n",
    "\texcept:\n",
    "\t\t# Generate Dictionary\n",
    "\t\tword_dict = generate_dictionary(files, labels)\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "def predict(files, word_dict):\n",
    "\t\"\"\"Predicts values using the word dictionary and returns a list of predictions\"\"\"\n",
    "\tpredictions = []\n",
    "\tfor file in tqdm(files):\n",
    "\t\t#Read and extract mail contents\n",
    "\t\ttry:\n",
    "\t\t\tmail = extract_content(file)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Corrupted File {}\" . format(file))\n",
    "\t\t\n",
    "\t\t# Prepare data\n",
    "\t\tmail = preprocess(mail)\n",
    "\t\tword_list = [s for s in mail.split()]\n",
    "\t\tword_set = set(word_list)\n",
    "\n",
    "\t\tn = 0\n",
    "\t\tspaminess_list = []\n",
    "\t\tfor word in word_set:\n",
    "\t\t\tif word not in word_dict:\n",
    "\t\t\t\tcontinue              \t\t\t\t\t\t# Ignore new words (for now)\n",
    "\t\t\t\tspaminess = 0.6       \t\t\t\t\t\t# Or... assume it is slightly spam ( Gives better FP, but lower f-score)\n",
    "\t\t\telse:\n",
    "\t\t\t\tspaminess = word_dict[word]['spaminess']\n",
    "\t\t\t\tif spaminess < 0.6 and spaminess > 0.4:\n",
    "\t\t\t\t\tcontinue                                #ignore the word if spaminess is neutral\n",
    "\t\t\tspaminess_list.append(spaminess)\n",
    "\n",
    "\t\t# Adding up all the word probabilities\n",
    "\t\tfor spaminess in spaminess_list:\n",
    "\t\t\tn +=  (math.log(1-spaminess) - math.log(spaminess))\n",
    "\t\tprobability = 1 / (1 + Decimal(math.e) ** Decimal(n))\n",
    "\t\t\n",
    "\t\t# Predicting \n",
    "\t\tif probability > 0.8:\n",
    "\t\t\tprediction = 0\n",
    "\t\telse:\n",
    "\t\t\tprediction = 1\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn predictions\n",
    "\n",
    "def binarylabel(value):\n",
    "\n",
    "\treturn (1 if value == \"spam\" else 0)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\t# Default paths for all the inputs. Overrided if script not in the same locations as them.\n",
    "# \ttrain = './TRAINING'\n",
    "# \ttest = './TESTING'\n",
    "\tmaildump = './trec07p/data'\n",
    "\tspam = './trec07p/full/index'\n",
    "\n",
    "\t# Getting user input if defaults are not valid\n",
    "\tprint(\"Please make sure the script is in the same directory as the Training and testing folders.\")\n",
    "\tif not (os.path.isdir(maildump) and os.path.exists(spam)):\n",
    "\t\tprint(\"Testing and training datasets not found: \")\n",
    "\t\ttrain = input(\"Enter training dataset path: \")\n",
    "\t\ttest = input(\"Enter testing dataset path: \")\n",
    "\t\tspam = input(\"Enter labels file path: \")\n",
    "\tMIN_SAMPLE_SIZE = 3000 # 3000\n",
    "\tMAX_SAMPLE_SIZE = 3700 # 3200\n",
    "\t# Getting training and testing files\n",
    "# \ttrain_files = sorted([os.path.join(train, file) for file in os.listdir(train)])[:3000]\n",
    "# \ttest_files = sorted([os.path.join(test, file) for file in os.listdir(test)])\n",
    "# \tfiles = train_files + test_files\n",
    "\tfiles = sorted([os.path.join(maildump, file) for file in os.listdir(maildump)])\n",
    "\ttrain_files = files[:MIN_SAMPLE_SIZE]\n",
    "\ttest_files = files[MIN_SAMPLE_SIZE:MAX_SAMPLE_SIZE]\n",
    "\tprint(\"Found the datasets.\")\n",
    "\t\n",
    "\t# Spam labels\n",
    "\twith open(spam, 'r') as f:\n",
    "\t\tlabels = [int(binarylabel(line.split()[0])) for line in f.readlines()]\n",
    "\ttrain_labels = labels[:MIN_SAMPLE_SIZE]\n",
    "\ttest_labels = labels[MIN_SAMPLE_SIZE:MAX_SAMPLE_SIZE]\n",
    "\n",
    "\n",
    "\t# Training our model\n",
    "\tprint(\"Training the model...\")\n",
    "\tword_dict = training(train_files, train_labels)\n",
    "\t\n",
    "\t# Predicting labels for both training and testing data.\n",
    "\tprint(\"Testing on both training and testing datasets...\")\n",
    "\tpredictions = predict(files, word_dict )\n",
    "\ttrain_predictions = predictions[:MIN_SAMPLE_SIZE]\n",
    "\ttest_predictions = predictions[MIN_SAMPLE_SIZE:MAX_SAMPLE_SIZE]\n",
    "\n",
    "\t# Get respective scores\n",
    "\ttest_scores = get_scores(test_labels, test_predictions)\n",
    "\ttrain_scores = get_scores(train_labels, train_predictions)\n",
    "\tcombined_scores = get_scores(labels, predictions)\n",
    "\n",
    "\t# Output results onto the console\n",
    "\tprint(\"\\nTraining Scores:\")\n",
    "\tfor key, value in sorted(train_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\tprint(\"\\nTesting Scores: \")\n",
    "\tfor key, value in sorted(test_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\tprint(\"\\nCombined Scores: \")\n",
    "\tfor key, value in sorted(combined_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\n",
    "\t# Creating a results file. Pandas object is used to help format our output.\n",
    "\tmails = {}\n",
    "\tmails['files'] = [os.path.split(file)[1] for file in files]\n",
    "\tmails['labels'] = labels\n",
    "\tmails['predictions'] = predictions\n",
    "\tdf = pd.DataFrame(mails)\n",
    "\tdf['result'] = np.where(df['predictions'] == df['labels'], \"CORRECT\", \"WRONG\")\n",
    "\tdf.set_index('files', inplace=True)\n",
    "\twith open('NBresults.txt', 'w') as f:\n",
    "\t\tf.write(df.to_string())\n",
    "\tprint(\"Results file created: {}\" . format(os.path.abspath('NBresults.txt')))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "255px",
    "left": "868.818px",
    "right": "20px",
    "top": "165px",
    "width": "351px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
