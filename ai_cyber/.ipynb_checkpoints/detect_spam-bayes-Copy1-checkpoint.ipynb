{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.python.keras.preprocessing  import  image \n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Dense,Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from IPython.display import display\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from numpy.random import seed\n",
    "from shutil import copyfile\n",
    "from time import time\n",
    "tf.set_random_seed(1)\n",
    "seed(1)\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# dimensions of our images.\n",
    "image_width, image_height = 32, 32\n",
    "WORK_DIRECTORY=\"./trec07p\"\n",
    "os.chdir(WORK_DIRECTORY) \n",
    "train_data_dir      = 'KaggleCatDog/train'\n",
    "validation_data_dir = 'KaggleCatDog/validation'\n",
    "weightsPath= \"weights/\"\n",
    "modelPath= \"model/\"\n",
    "weightsFilePath= \"weights/weightscatsdogs.h5\"\n",
    "modelFilePath= \"model/modelcatsdogs.json\"\n",
    "path = WORK_DIRECTORY+\"UserData/\"\n",
    "nb_train_samples = 24158\n",
    "nb_validation_samples = 1052\n",
    "epochs =2\n",
    "batch_size = 25\n",
    "num_classes =2\n",
    "input_shape = (image_width, image_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def HouseKeeping(pathName):\n",
    "    pathName = WORK_DIRECTORY+ pathName\n",
    "    if os.path.exists(pathName):\n",
    "        shutil.rmtree(pathName)\n",
    "    if not os.path.exists(pathName):\n",
    "        os.makedirs(pathName)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   rotation_range=90.,\n",
    "                                   zoom_range=0.3,\n",
    "                                   shear_range=0.3)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    " \n",
    "#conv2d_1  32 filters of shape 3 X 3\n",
    "#Parameters = 896 = ((Filter width * Filter Breadth * Channels) + 1) * Number of Filters\n",
    "#((3 X 3 X 3 )+1) *32 ) = 896\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))     \n",
    "\n",
    "# Filter Count * 4\n",
    "# 32 * 4 = 128\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Activation('relu'))\n",
    "#max_pooling2d_1\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "\n",
    "#conv2d_2  64 filters of shape 3 X 3   \n",
    "# (3 * 3 * 32 + 1) * 64 = 18496\n",
    "model.add(Conv2D(64, (3, 3)))           \n",
    "\n",
    "# Filter Count * 4\n",
    "# 64 * 4 = 256\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#max_pooling2d_2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "#conv2d_3  128 filters of shape 3 X 3    \n",
    "# ( 3 * 3  * 64 +1)*128  = 73856\n",
    "model.add(Conv2D(128, (3, 3)))        \n",
    "\n",
    "# Filter Count * 4\n",
    "# 128 * 4 = 512\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "#max_pooling2d_3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    " \n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# last Count = 512\n",
    "# (last count + 1) * DenseCount\n",
    "#(512 + 1)* 100 = 51300\n",
    "model.add(Dense(100, activation='elu'))\n",
    " \n",
    "# (100+1)*1 = 101\n",
    "model.add(Dense(1,   activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "HouseKeeping(weightsPath)\n",
    "HouseKeeping(modelPath)\n",
    "model_json = model.to_json()\n",
    "with open(modelFilePath, \"w\") as json_file:\n",
    "    json_file.write(model_json) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#+\"{epoch:03d}-{val_acc:3f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=weightsFilePath,\n",
    "                               verbose=1, \n",
    "                               monitor='val_acc',\n",
    "                               save_best_only=True,\n",
    "                               mode='max')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    history=model.fit_generator(\n",
    "        train_generator,\n",
    "        verbose=1,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['acc','val_acc'])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files = os.listdir(path)\n",
    "\n",
    "for file in files:\n",
    "   \n",
    "    t_image= image.load_img(path + file )\n",
    "    print(path + file )\n",
    "    test_image = image.load_img(path + file, target_size=(image_height, image_width))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    pred = model.predict_on_batch(test_image)\n",
    "    print(pred)\n",
    "    if pred >= 1.0:\n",
    "        print(\"Dog\")\n",
    "    else:\n",
    "        print(\"Cat\")\n",
    "\n",
    "    plt.imshow(t_image)\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf \n",
    "import os\n",
    "\n",
    "# Function to encode string features\n",
    "def encode_features_and_labels(training, testing):\n",
    "\n",
    "\t# To encode string  labels into numbers\n",
    "\tle = LabelEncoder()\n",
    "\n",
    "\t# Creates new dummy columns from each unique string in a particulat feature\n",
    "\ttraining = pd.get_dummies(data=training, columns=['proto', 'service', 'state'])\n",
    "\ttesting = pd.get_dummies(data=testing, columns=['proto', 'service', 'state'])\n",
    "\n",
    "\t# Making sure that the training features are same as testing features.\n",
    "\t# The training dataset has more unique protocols and states, therefore number \\\n",
    "\t# of dummy columns will be different in both. We make it the same.\n",
    "\ttraincols = list(training.columns.values)\n",
    "\ttestcols = list(testing.columns.values)\n",
    "\n",
    "\t# For those in training but not in testing\n",
    "\tfor col in traincols:\n",
    "\t\t# If a column is missing in the testing dataset, we add it\n",
    "\t\tif col not in testcols:\n",
    "\t\t\ttesting[col] = 0\n",
    "\t\t\ttestcols.append(col)\n",
    "\t# For those in testing but not in training\n",
    "\tfor col in testcols:\n",
    "\t\tif col not in traincols:\n",
    "\t\t\ttraining[col] = 0\n",
    "\t\t\ttraincols.append(col)\n",
    "\n",
    "\n",
    "\t# Moving the labels and categories to the end and making sure features are in the same order\n",
    "\ttraincols.pop(traincols.index('attack_cat'))\n",
    "\ttraincols.pop(traincols.index('label'))\n",
    "\ttraining = training[traincols+['attack_cat', 'label']]\n",
    "\ttesting = testing[traincols+['attack_cat', 'label']]\n",
    "\n",
    "\t# Encoding the category names into numbers so that they can be one hot encoded later.\n",
    "\ttraining['attack_cat'] = le.fit_transform(training['attack_cat'])\n",
    "\ttesting['attack_cat'] = le.fit_transform(testing['attack_cat'])\n",
    "\n",
    "\t# Returning modified dataframes and the vocabulary of labels for inverse transform\n",
    "\treturn (training, testing, le)\n",
    "\n",
    "# Parameters\n",
    "training_epochs = 20\n",
    "batch_size = 9\n",
    "start_rate = 0.0002\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 50 # 2nd layer number of neurons\n",
    "n_features = 196 # There are 194 different features for each packet.\n",
    "n_classes = 10 # There are 9 different types of malicious packets + Normal\n",
    "\n",
    "########### Defining tensorflow computational graph ###########\n",
    "\n",
    "# tf Graph input\n",
    "# Features\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "# Labels\n",
    "Y = tf.placeholder(tf.int32, [None,])\n",
    "# decay step for learning rate decay\n",
    "decay_step = tf.placeholder(tf.int32)\n",
    "\n",
    "\n",
    "# Create model\n",
    "def deep_neural_network(x):\n",
    "\n",
    "    # Hidden fully connected layer with 100 neurons\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1, activation=tf.nn.relu)\n",
    "    # Hidden fully connected layer with 50 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.layers.dense(layer_2, n_classes)\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = deep_neural_network(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# Converting categories into one hot labels\n",
    "labels = tf.one_hot(indices=tf.cast(Y, tf.int32), depth=n_classes)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    \t\t\t\t\tlogits=logits, labels=labels))\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Using a learning rate which has polynomial decay\n",
    "starter_learning_rate = start_rate\n",
    "end_learning_rate = 0.00005 # we will use a polynomial decay to reach learning this learning rate.29\n",
    "decay_steps = decay_step\n",
    "learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "                                          decay_steps, end_learning_rate,\n",
    "                                          power=0.5)\n",
    "# Using adam optimizer to reduce loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Model for testing\n",
    "pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "\n",
    "# Model for prediction: Used to just return predicted values\n",
    "prediction=tf.argmax(pred,1)\n",
    "\n",
    "########## END of model ############\n",
    "\n",
    "########## Reading and processing input datasets #########\n",
    "\n",
    "# Default values. \n",
    "train_set = 'm-set.csv'\n",
    "test_set = 'UNSW_NB15_testing-set.csv'\n",
    "\n",
    "# Comment if you need to hardcode path\n",
    "# train_set = input(\"Enter training dataset: \")\n",
    "# test_set = input(\"Enter testing dataset: \")\n",
    "# if not os.path.exists(train_set) or not os.path.exists(test_set):\n",
    "# \tprint(\"Files not found\")\n",
    "# \texit()\n",
    "# Read data using pandas\n",
    "training = pd.read_csv(train_set, index_col='id')\n",
    "testing = pd.read_csv(test_set, index_col='id')\n",
    "\n",
    "# Encoding string columns\n",
    "training, testing, le = encode_features_and_labels(training, testing)\n",
    "\n",
    "# Normalising all numerical features:\n",
    "cols_to_normalise = list(training.columns.values)[:39]\n",
    "training[cols_to_normalise] = training[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "testing[cols_to_normalise] = testing[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "######## End of preprocessing #######\n",
    "\n",
    "######## Training and testing #########\n",
    "\n",
    "def get_accuracy(df):\n",
    "\n",
    "\t# Calculate accuracy for label classification\n",
    "\tcategories = prediction.eval(feed_dict={X: df.iloc[:, 0:-2]}) # Getting back the predictions\n",
    "\n",
    "\t# Function to convert categories back into binary labels\n",
    "\tf = lambda x: 0 if le.inverse_transform(x) == \"Normal\" else 1\n",
    "\n",
    "\t# Prepating the necessary predictions and labels for comparision; converting categories to normal/malicious\n",
    "\tbinary_prediction = np.fromiter((f(xi) for xi in categories), categories.dtype, count=len(categories))\n",
    "\tbinary_labels = df.iloc[:, -1].values\n",
    "\t\n",
    "\t# Compating predictions and labels to calculate accuracy\n",
    "\tcorrect_labels = tf.equal(binary_prediction, binary_labels)\n",
    "\tlabel_accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))\n",
    "\tresult = label_accuracy.eval()\n",
    "\tprint(\"Label accuracy: {:.2f}%\".format(result*100))\n",
    "\n",
    "\t# Calculate accuracy for category classification\n",
    "\tcorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\tresult = accuracy.eval({X: df.iloc[:, 0:-2], Y: df.iloc[:,-2]})\n",
    "\tprint(\"Category accuracy: {:.2f}%\".format(result*100))\n",
    "\n",
    "def train_and_test_model(training, testing):\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(init)\n",
    "\n",
    "\t\t# Training cycle\n",
    "\t\tfor epoch in range(training_epochs):\n",
    "\t\t\t# Shuffling dataset before training\n",
    "\t\t\tdf = training.sample(frac=1)\n",
    "\t\t\tavg_cost = 0.\n",
    "\t\t\ttotal_data = df.index.shape[0] \n",
    "\t\t\tnum_batches = total_data // batch_size + 1\n",
    "\t\t\ti = 0\n",
    "\t\t\t# Loop over all batches\n",
    "\t\t\twhile i < total_data:\n",
    "\t\t\t\tbatch_x = df.iloc[i:i+batch_size, 0:-2].values\n",
    "\t\t\t\tbatch_y = df.iloc[i:i+batch_size, -2].values # Last two columns are categories and labels\n",
    "\t\t\t\ti += batch_size\n",
    "\t\t\t\t# Run optimization op and cost op (to get loss value)\n",
    "\t\t\t\t_, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "\t\t\t\t                                                Y: batch_y,\n",
    "\t\t\t\t                                                decay_step: num_batches * training_epochs})\n",
    "\t\t\t\t# Compute average loss\n",
    "\t\t\t\tavg_cost += c / num_batches\n",
    "\t\t\t# Display logs per epoch step\n",
    "\t\t\tprint(\"Epoch: {:04} | Cost={:.9f}\".format(epoch+1, avg_cost))\n",
    "\t\t\tget_accuracy(testing)\n",
    "\t\t\tprint()\n",
    "\t\tprint(\"Training complete\")\n",
    "\n",
    "\t\tprint(\"Training results: \")\n",
    "\t\tget_accuracy(training)\n",
    "\t\tprint(\"Testing results: \")\n",
    "\t\tget_accuracy(testing)\n",
    "\n",
    "\n",
    "# Training the model after shuffling the data.\n",
    "train_and_test_model(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spam filter using Naive Bayes classifier\"\"\"\n",
    "\n",
    "\n",
    "import email.parser \n",
    "import os, sys, stat\n",
    "from tqdm import tqdm\n",
    "import re, cgi\n",
    "import math, pickle\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_content(filename):\n",
    "\t''' Extract the subject and payload from the .eml file.'''\n",
    "\twith open(filename, 'rb') as fp:\n",
    "\t\tmsg = email.message_from_bytes(fp.read())\n",
    "\tsub = msg.get('subject')\n",
    "\t#If it is a multipart message, get_payload returns a list of parts.\n",
    "\tif msg.is_multipart():\n",
    "\t\tpayload = msg.get_payload()[0]\t\n",
    "\t\tpayload = payload.as_bytes() #We will consider the body as bytes so it is easier to decode into a unicode string.\n",
    "\telse:\n",
    "\t\tpayload =  msg.get_payload()\n",
    "\treturn \"{}\\n{}\" . format(sub, payload)\n",
    "\n",
    "def get_text_from_email(mail):\n",
    "\t\"\"\" Removes html tags and punctuations.\"\"\"\n",
    "\ttag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
    "\n",
    "\t# Remove well-formed tags, fixing mistakes by legitimate users\n",
    "\tmail = tag_re.sub('', mail)\n",
    "\n",
    "\t# Clean up anything else by escaping\n",
    "\tmail = cgi.escape(mail)\n",
    "\t\n",
    "\tmail = re.sub(r'([\\\\][n|t|x])', ' ', mail)                           #Removes \\n\\t\\b strings\n",
    "\tmail = re.sub(r'[=*/&;.,/\\\" ?:<>\\[\\]\\(\\)\\{\\}\\|%#`~\\\\]', ' ', mail)   #Removes punctuations\n",
    "\tmail = re.sub(r'[- _=+]{2,}|(?=\\s)[-_]|[-_](?=\\s)', ' ', mail)       #Removes unnecessary hiphens and underscores\n",
    "\tmail = re.sub(r'[\\d]', ' ', mail)                                    #Revoves all digits\n",
    "\tmail = re.sub(r'[\\'!=+]', '', mail)                                  #Replaces these punctuations with null string\n",
    "\treturn mail.lower()\n",
    "\n",
    "\n",
    "def preprocess(mail):\n",
    "\t\"\"\"Preprocess data\"\"\"\n",
    "\t# Currently just one preprocessing step.\n",
    "\tmail = get_text_from_email(mail)\n",
    "\treturn mail\n",
    "\n",
    "\n",
    "def add_words_to_dict(word_set, word_dict, ham):\n",
    "\t\"\"\"Checks if the word is presnt or not and increments its respective value\"\"\"\n",
    "\tfor word in word_set:\n",
    "\t\tif word not in word_dict:\n",
    "\t\t\tword_dict[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "\t\tif ham:\n",
    "\t\t\tword_dict[word]['ham_count'] = word_dict[word]['ham_count'] + 1\n",
    "\t\telse:\n",
    "\t\t\tword_dict[word]['spam_count'] = word_dict[word]['spam_count'] + 1 \n",
    "\n",
    "def calculate_spaminess(word, word_dict, total_ham, total_spam):\n",
    "\t\"\"\" Calculate the probability of a message being spam provided that the word is present.\"\"\"\n",
    "\n",
    "\tpr_s, pr_h = 0.5, 0.5  #Assumming equal probability for both ham and spam\n",
    "\tthreshold = 2   #Strength factor to handle rare words\n",
    "\ttotal_occurance = word_dict[word]['spam_count'] + word_dict[word]['ham_count']  #Total number of times the word has occured in both ham and spam\n",
    "\tfreq_s = word_dict[word]['spam_count'] / total_spam \n",
    "\tfreq_h = word_dict[word]['ham_count'] / total_ham\n",
    "\tspamminess = (freq_s * pr_s) / (freq_s * pr_s + freq_h * pr_h)  #The probability that a given mail is spam, provided that this word is present.\n",
    "\tcorrected_spaminess = (0.3 * threshold + total_occurance * spamminess) / (threshold + total_occurance)  #Considering the strength factor.\n",
    "\tword_dict[word]['spaminess'] = corrected_spaminess   \n",
    "\n",
    "def generate_dictionary(files, labels):\n",
    "\t\"\"\"Generates a dictionary of all the words in both ham and spam mails\"\"\"\n",
    "\t#Initializing variables\n",
    "\titerator = 0\n",
    "\tword_dict = {}\n",
    "\ttotal_spam = 0\n",
    "\ttotal_ham = 0\n",
    "\n",
    "\tfor file in tqdm(files):\n",
    "\t\t#Read and extract mail contents\n",
    "\t\ttry:\n",
    "\t\t\tmail = extract_content(file)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Corrupted File {}\" . format(file))\n",
    "\t\t# Prepare data\n",
    "\t\tmail = preprocess(mail)\n",
    "\t\tword_list = [s for s in mail.split()]\n",
    "\t\tword_set = set(word_list)\n",
    "\n",
    "\t\t# Incrementing HAM/SPAM count\n",
    "\t\tham = (True if (labels[iterator].split()[0]) == \"spam\" else False)\n",
    "\t\tif ham:\n",
    "\t\t\ttotal_ham += 1\n",
    "\t\telse:\n",
    "\t\t\ttotal_spam += 1\n",
    "\n",
    "\t\tadd_words_to_dict(word_set, word_dict, ham)\n",
    "\t\titerator += 1\n",
    "\tfor word in word_dict:\n",
    "\t\tcalculate_spaminess(word, word_dict, total_ham, total_spam)\n",
    "\twith open('word_dict.pickle', 'wb') as f:\n",
    "\t\tpickle.dump(word_dict, f)\n",
    "\treturn word_dict\n",
    "\n",
    "def get_scores(expected, predicted):\n",
    "\t\"\"\" Compares predicted and expected values and returns various metrics.\"\"\"\n",
    "\tscores = {}\n",
    "\t# _ implies we do not care about that metric.\n",
    "\t_, scores['False Positives'], scores['False Negatives'], _= confusion_matrix(expected, predicted).ravel()\n",
    "\tscores['Precision'], scores['Recall'], scores['F_score'], _= precision_recall_fscore_support(expected, predicted, average='macro')\n",
    "\treturn scores\n",
    "\n",
    "def training(files, labels):\n",
    "\t\"\"\"Trains the model and returns a word dictionary\"\"\"\n",
    "\ttry:\n",
    "\t\twith open('word_dict.pickle', 'rb') as f:\n",
    "\t\t\tprint(\"Found pickle file. Skipping training\")\n",
    "\t\t\tword_dict = pickle.load(f)\n",
    "\texcept:\n",
    "\t\t# Generate Dictionary\n",
    "\t\tword_dict = generate_dictionary(files, labels)\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "def predict(files, word_dict):\n",
    "\t\"\"\"Predicts values using the word dictionary and returns a list of predictions\"\"\"\n",
    "\tpredictions = []\n",
    "\tfor file in tqdm(files):\n",
    "\t\t#Read and extract mail contents\n",
    "\t\ttry:\n",
    "\t\t\tmail = extract_content(file)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Corrupted File {}\" . format(file))\n",
    "\t\t\n",
    "\t\t# Prepare data\n",
    "\t\tmail = preprocess(mail)\n",
    "\t\tword_list = [s for s in mail.split()]\n",
    "\t\tword_set = set(word_list)\n",
    "\n",
    "\t\tn = 0\n",
    "\t\tspaminess_list = []\n",
    "\t\tfor word in word_set:\n",
    "\t\t\tif word not in word_dict:\n",
    "\t\t\t\tcontinue              \t\t\t\t\t\t# Ignore new words (for now)\n",
    "\t\t\t\tspaminess = 0.6       \t\t\t\t\t\t# Or... assume it is slightly spam ( Gives better FP, but lower f-score)\n",
    "\t\t\telse:\n",
    "\t\t\t\tspaminess = word_dict[word]['spaminess']\n",
    "\t\t\t\tif spaminess < 0.6 and spaminess > 0.4:\n",
    "\t\t\t\t\tcontinue                                #ignore the word if spaminess is neutral\n",
    "\t\t\tspaminess_list.append(spaminess)\n",
    "\n",
    "\t\t# Adding up all the word probabilities\n",
    "\t\tfor spaminess in spaminess_list:\n",
    "\t\t\tn +=  (math.log(1-spaminess) - math.log(spaminess))\n",
    "\t\tprobability = 1 / (1 + Decimal(math.e) ** Decimal(n))\n",
    "\t\t\n",
    "\t\t# Predicting \n",
    "\t\tif probability > 0.8:\n",
    "\t\t\tprediction = '0'\n",
    "\t\telse:\n",
    "\t\t\tprediction = '1'\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn predictions\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\t# Default paths for all the inputs. Overrided if script not in the same locations as them.\n",
    "\ttrain = './trec07p/data'\n",
    "\ttest = './trec07p/data'\n",
    "\tspam = './trec07p/full'\n",
    "\n",
    "\t# Getting user input if defaults are not valid\n",
    "\tprint(\"Please make sure the script is in the same directory as the Training and testing folders.\")\n",
    "\tif not (os.path.isdir(train) and os.path.isdir(test) and os.path.exists(spam)):\n",
    "\t\tprint(\"Testing and training datasets not found: \")\n",
    "\t\ttrain = input(\"Enter training dataset path: \")\n",
    "\t\ttest = input(\"Enter testing dataset path: \")\n",
    "\t\tspam = input(\"Enter labels file path: \")\n",
    "\t\n",
    "\t# Getting training and testing files\n",
    "\ttrain_files = sorted([os.path.join(train, file) for file in os.listdir(train)])[:3000]\n",
    "\ttest_files = sorted([os.path.join(test, file) for file in os.listdir(test)])\n",
    "\tfiles = train_files + test_files\n",
    "\tprint(\"Found the datasets.\")\n",
    "\t\n",
    "\t# Spam labels\n",
    "\twith open(spam, 'r') as f:\n",
    "\t\tlabels = [line.split()[0] for line in f.readlines()]\n",
    "\ttrain_labels = labels[:3000]\n",
    "\ttest_labels = labels[3000:]\n",
    "\n",
    "\t# Training our model\n",
    "\tprint(\"Training the model...\")\n",
    "\tword_dict = training(train_files, train_labels)\n",
    "\t\n",
    "\t# Predicting labels for both training and testing data.\n",
    "\tprint(\"Testing on both training and testing datasets...\")\n",
    "\tpredictions = predict(files, word_dict )\n",
    "\ttrain_predictions = predictions[:3000]\n",
    "\ttest_predictions = predictions[3000:]\n",
    "\n",
    "\t# Get respective scores\n",
    "\ttest_scores = get_scores(test_labels, test_predictions)\n",
    "\ttrain_scores = get_scores(train_labels, train_predictions)\n",
    "\tcombined_scores = get_scores(labels, predictions)\n",
    "\n",
    "\t# Output results onto the console\n",
    "\tprint(\"\\nTraining Scores:\")\n",
    "\tfor key, value in sorted(train_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\tprint(\"\\nTesting Scores: \")\n",
    "\tfor key, value in sorted(test_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\tprint(\"\\nCombined Scores: \")\n",
    "\tfor key, value in sorted(combined_scores.items()):\n",
    "\t\tprint(\"{:15} : {:.5}\" .format(key, float(value)))\n",
    "\n",
    "\t# Creating a results file. Pandas object is used to help format our output.\n",
    "\tmails = {}\n",
    "\tmails['files'] = [os.path.split(file)[1] for file in files]\n",
    "\tmails['labels'] = labels\n",
    "\tmails['predictions'] = predictions\n",
    "\tdf = pd.DataFrame(mails)\n",
    "\tdf['result'] = np.where(df['predictions'] == df['labels'], \"CORRECT\", \"WRONG\")\n",
    "\tdf.set_index('files', inplace=True)\n",
    "\twith open('NBresults.txt', 'w') as f:\n",
    "\t\tf.write(df.to_string())\n",
    "\tprint(\"Results file created: {}\" . format(os.path.abspath('NBresults.txt')))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
